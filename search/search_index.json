{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"Open Source Voice Interface Kiwi Voice <p>     ML wake word detection, speaker identification, voice\u2011gated security,     5\u00a0TTS engines, 15\u00a0languages, and a real\u2011time web\u00a0dashboard \u2014     for\u00a0your own AI\u00a0stack.   </p> Get Started GitHub Features \ud83d\udde3\ufe0f Wake Word Detection Text fuzzy matching or OpenWakeWord ML \u2014 ONNX model, ~80ms latency, ~2% CPU. Built-in models or train your own. \ud83c\udfad Speaker Identification Voiceprint recognition via pyannote embeddings. Priority hierarchy: Owner \u2192 Friend \u2192 Guest \u2192 Blocked. \ud83d\udd10 Two-Layer Security Pre-LLM dangerous command detector + post-LLM exec approval. Telegram notifications for non-owner actions. \ud83d\udd0a 5 TTS Providers ElevenLabs, Kokoro ONNX, Piper, Qwen3-TTS. Streaming sentence-aware chunking with barge-in support. \ud83d\udcca Web Dashboard &amp; API Glassmorphism dark dashboard with live status, event log, personalities, speaker management, and browser mic. \ud83c\udfe0 Home Assistant Bidirectional integration. Control Kiwi from HA dashboard, control your smart home by voice through Kiwi. \ud83c\udf0d 15 Languages Full i18n with YAML locales. All strings, voice commands, wake word variants, and security patterns per-language. \ud83c\udfad Personality System 5 built-in \"souls\" \u2014 switch by voice, API, or dashboard. NSFW routes to a separate isolated LLM session."},{"location":"#how-it-works","title":"How it works","text":"<p>Kiwi Voice turns your OpenClaw agent into a hands-free assistant. It captures audio from your microphone (or directly from the browser), detects the wake word, transcribes speech locally, identifies who is speaking, enforces security policies, sends the command to any LLM through OpenClaw's WebSocket gateway, and speaks the response back \u2014 all in a continuous loop.</p> <pre><code>You:  \"Kiwi, turn on the lights in the bedroom\"\n\nKiwi: [identifies speaker as Owner \u2192 full access]\n      [sends to OpenClaw \u2192 routes to Home Assistant]\n      \"Done, the bedroom lights are on.\"\n</code></pre> <p>Think Alexa or Siri, but self-hosted, privacy-first, and plugged into your own AI stack.</p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>git clone https://github.com/ekleziast/kiwi-voice.git\ncd kiwi-voice\npip install -r requirements.txt\ncp .env.example .env\npython -m kiwi\n</code></pre> <p>Open http://localhost:7789 for the web dashboard.</p> <p>Full installation guide \u2192</p>"},{"location":"#architecture","title":"Architecture","text":"<pre><code>Mic (24kHz) / Browser WebSocket \u2192 Audio Pipeline (Silero VAD + energy detection)\n  \u2192 Wake Word (OpenWakeWord ML or text fuzzy match)\n  \u2192 Faster Whisper STT (or MLX Whisper on Apple Silicon)\n  \u2192 Speaker ID (pyannote embeddings) \u2192 Priority Gate (Owner/Friend/Guest/Blocked)\n  \u2192 Voice Security (dangerous command regex \u2192 Telegram approval)\n  \u2192 OpenClaw Gateway (WebSocket v3)\n  \u2192 LLM response stream (delta \u2192 sentence chunking)\n  \u2192 Streaming TTS (Kokoro/Piper/Qwen3/ElevenLabs) \u2192 Speaker output + browser playback\n  \u2192 Barge-in detection \u2192 back to listening\n</code></pre> <p>Architecture deep dive \u2192</p>"},{"location":"api/rest/","title":"REST API","text":"<p>Base URL: <code>http://localhost:7789</code></p> <p>All endpoints return JSON. The API is served by the same aiohttp server as the dashboard.</p>"},{"location":"api/rest/#status-config","title":"Status &amp; Config","text":""},{"location":"api/rest/#get-apistatus","title":"<code>GET /api/status</code>","text":"<p>Returns current service state and metrics.</p> <pre><code>{\n  \"state\": \"LISTENING\",\n  \"language\": \"en\",\n  \"tts_provider\": \"kokoro\",\n  \"is_speaking\": false,\n  \"is_processing\": false,\n  \"is_running\": true,\n  \"uptime_seconds\": 3600,\n  \"active_speaker\": \"Owner\",\n  \"active_soul\": \"default\",\n  \"homeassistant_connected\": true\n}\n</code></pre>"},{"location":"api/rest/#get-apiconfig","title":"<code>GET /api/config</code>","text":"<p>Returns current configuration (safe fields only, no secrets).</p> <pre><code>{\n  \"language\": \"en\",\n  \"tts_provider\": \"kokoro\",\n  \"tts_qwen_backend\": \"local\",\n  \"tts_voice\": \"af_heart\",\n  \"stt_model\": \"large\",\n  \"stt_device\": \"cuda\",\n  \"wake_word\": \"kiwi\",\n  \"wake_word_engine\": \"openwakeword\"\n}\n</code></pre>"},{"location":"api/rest/#patch-apiconfig","title":"<code>PATCH /api/config</code>","text":"<p>Update configuration at runtime.</p> <p>Request:</p> <pre><code>{\n  \"language\": \"ru\",\n  \"wake_word\": \"jarvis\",\n  \"tts_default_style\": \"cheerful\"\n}\n</code></pre> <p>Response:</p> <pre><code>{\"updated\": {\"language\": \"ru\"}}\n</code></pre>"},{"location":"api/rest/#speakers","title":"Speakers","text":""},{"location":"api/rest/#get-apispeakers","title":"<code>GET /api/speakers</code>","text":"<p>List all known speaker profiles.</p> <pre><code>{\n  \"speakers\": [\n    {\n      \"id\": \"spk_001\",\n      \"name\": \"Owner\",\n      \"priority\": 0,\n      \"is_blocked\": false,\n      \"auto_learned\": false,\n      \"sample_count\": 42,\n      \"last_seen\": \"2026-02-25T10:30:00\"\n    }\n  ]\n}\n</code></pre>"},{"location":"api/rest/#delete-apispeakersspeaker_id","title":"<code>DELETE /api/speakers/{speaker_id}</code>","text":"<p>Remove a speaker profile.</p> <pre><code>{\"deleted\": \"spk_001\"}\n</code></pre>"},{"location":"api/rest/#post-apispeakersspeaker_idblock","title":"<code>POST /api/speakers/{speaker_id}/block</code>","text":"<p>Block a speaker.</p> <pre><code>{\"blocked\": \"spk_001\"}\n</code></pre>"},{"location":"api/rest/#post-apispeakersspeaker_idunblock","title":"<code>POST /api/speakers/{speaker_id}/unblock</code>","text":"<p>Unblock a speaker.</p> <pre><code>{\"unblocked\": \"spk_001\"}\n</code></pre>"},{"location":"api/rest/#languages","title":"Languages","text":""},{"location":"api/rest/#get-apilanguages","title":"<code>GET /api/languages</code>","text":"<pre><code>{\n  \"current\": \"en\",\n  \"available\": [\"en\", \"ru\", \"es\", \"pt\", \"fr\", \"it\", \"de\", \"tr\", \"pl\", \"zh\", \"ja\", \"ko\", \"hi\", \"ar\", \"id\"]\n}\n</code></pre>"},{"location":"api/rest/#post-apilanguage","title":"<code>POST /api/language</code>","text":"<p>Switch language at runtime.</p> <p>Request: <code>{\"language\": \"ru\"}</code></p> <p>Response: <code>{\"language\": \"ru\"}</code></p>"},{"location":"api/rest/#souls-personalities","title":"Souls (Personalities)","text":""},{"location":"api/rest/#get-apisouls","title":"<code>GET /api/souls</code>","text":"<p>List all available personalities.</p> <pre><code>{\n  \"souls\": [\n    {\"id\": \"default\", \"name\": \"Default\", \"description\": \"Balanced assistant\", \"nsfw\": false},\n    {\"id\": \"comedian\", \"name\": \"Comedian\", \"description\": \"Funny and witty\", \"nsfw\": false},\n    {\"id\": \"siren\", \"name\": \"Siren\", \"description\": \"Flirty 18+\", \"nsfw\": true}\n  ]\n}\n</code></pre>"},{"location":"api/rest/#get-apisoulcurrent","title":"<code>GET /api/soul/current</code>","text":"<pre><code>{\n  \"id\": \"default\",\n  \"name\": \"Default\",\n  \"description\": \"...\",\n  \"nsfw\": false,\n  \"model\": \"claude-sonnet\"\n}\n</code></pre>"},{"location":"api/rest/#post-apisoul","title":"<code>POST /api/soul</code>","text":"<p>Switch personality.</p> <p>Request: <code>{\"soul\": \"comedian\"}</code></p> <p>Response:</p> <pre><code>{\n  \"soul\": \"comedian\",\n  \"name\": \"Comedian\",\n  \"nsfw\": false,\n  \"model\": \"claude-sonnet\"\n}\n</code></pre>"},{"location":"api/rest/#tts","title":"TTS","text":""},{"location":"api/rest/#post-apittstest","title":"<code>POST /api/tts/test</code>","text":"<p>Speak a test phrase through the current TTS provider.</p> <p>Request: <code>{\"text\": \"Hello, I am Kiwi!\"}</code> (optional \u2014 uses default if omitted)</p> <p>Response:</p> <pre><code>{\"status\": \"speaking\", \"text\": \"Hello, I am Kiwi!\"}\n</code></pre>"},{"location":"api/rest/#controls","title":"Controls","text":""},{"location":"api/rest/#post-apistop","title":"<code>POST /api/stop</code>","text":"<p>Stop current TTS playback.</p> <pre><code>{\"status\": \"stopped\"}\n</code></pre>"},{"location":"api/rest/#post-apireset-context","title":"<code>POST /api/reset-context</code>","text":"<p>Reset conversation context (clears LLM history).</p> <pre><code>{\"status\": \"context_reset\"}\n</code></pre>"},{"location":"api/rest/#post-apirestart","title":"<code>POST /api/restart</code>","text":"<p>Restart the service.</p> <pre><code>{\"status\": \"restarting\"}\n</code></pre>"},{"location":"api/rest/#post-apishutdown","title":"<code>POST /api/shutdown</code>","text":"<p>Shutdown the service.</p> <pre><code>{\"status\": \"shutting_down\"}\n</code></pre>"},{"location":"api/rest/#home-assistant","title":"Home Assistant","text":""},{"location":"api/rest/#get-apihomeassistantstatus","title":"<code>GET /api/homeassistant/status</code>","text":"<pre><code>{\"enabled\": true, \"connected\": true}\n</code></pre>"},{"location":"api/rest/#post-apihomeassistantcommand","title":"<code>POST /api/homeassistant/command</code>","text":"<p>Send a voice command to Home Assistant via the Conversation API.</p> <p>Request:</p> <pre><code>{\"text\": \"turn on bedroom lights\", \"language\": \"en\"}\n</code></pre> <p>Response:</p> <pre><code>{\n  \"response\": \"Done, bedroom lights are on.\",\n  \"command\": \"turn on bedroom lights\"\n}\n</code></pre>"},{"location":"api/rest/#web-audio","title":"Web Audio","text":""},{"location":"api/rest/#websocket-apiaudio","title":"<code>WebSocket /api/audio</code>","text":"<p>Bidirectional audio streaming for the browser microphone. See Web Microphone for protocol details.</p>"},{"location":"api/websocket/","title":"WebSocket Events","text":"<p>Kiwi exposes a real-time event stream via WebSocket for building custom integrations and UIs.</p>"},{"location":"api/websocket/#connection","title":"Connection","text":"<pre><code>ws://localhost:7789/api/events\n</code></pre> <p>Connect using any WebSocket client. The dashboard uses this endpoint for live updates.</p>"},{"location":"api/websocket/#javascript-example","title":"JavaScript Example","text":"<pre><code>const ws = new WebSocket('ws://localhost:7789/api/events');\n\nws.onmessage = (event) =&gt; {\n  const data = JSON.parse(event.data);\n  console.log(`[${data.event}]`, data.data);\n};\n</code></pre>"},{"location":"api/websocket/#python-example","title":"Python Example","text":"<pre><code>import websocket\nimport json\n\ndef on_message(ws, message):\n    event = json.loads(message)\n    print(f\"[{event['event']}] {event.get('data', {})}\")\n\nws = websocket.WebSocketApp(\n    \"ws://localhost:7789/api/events\",\n    on_message=on_message\n)\nws.run_forever()\n</code></pre>"},{"location":"api/websocket/#event-format","title":"Event Format","text":"<p>All events follow the same structure:</p> <pre><code>{\n  \"event\": \"WAKE_WORD_DETECTED\",\n  \"data\": {\"text\": \"kiwi\"},\n  \"timestamp\": 1709000000.0,\n  \"source\": \"listener\"\n}\n</code></pre> Field Type Description <code>event</code> string Event type name <code>data</code> object Event-specific payload <code>timestamp</code> float Unix timestamp <code>source</code> string Module that emitted the event"},{"location":"api/websocket/#event-types","title":"Event Types","text":""},{"location":"api/websocket/#core-pipeline","title":"Core Pipeline","text":"Event Data Description <code>STATE_CHANGED</code> <code>{\"old\": \"IDLE\", \"new\": \"LISTENING\"}</code> Assistant state transition <code>WAKE_WORD_DETECTED</code> <code>{\"text\": \"kiwi\"}</code> Wake word detected <code>SPEECH_RECOGNIZED</code> <code>{\"text\": \"turn on the lights\", \"language\": \"en\"}</code> STT transcription complete <code>SPEAKER_IDENTIFIED</code> <code>{\"name\": \"Owner\", \"priority\": 0, \"confidence\": 0.95}</code> Speaker identified by voiceprint"},{"location":"api/websocket/#llm","title":"LLM","text":"Event Data Description <code>LLM_TOKEN</code> <code>{\"token\": \"Hello\"}</code> Streaming LLM token <code>LLM_COMPLETE</code> <code>{\"text\": \"Hello! How can I help?\"}</code> Full LLM response complete"},{"location":"api/websocket/#tts","title":"TTS","text":"Event Data Description <code>TTS_STARTED</code> <code>{\"text\": \"Hello!\", \"provider\": \"kokoro\"}</code> TTS playback started <code>TTS_FINISHED</code> <code>{}</code> TTS playback finished"},{"location":"api/websocket/#security","title":"Security","text":"Event Data Description <code>APPROVAL_REQUESTED</code> <code>{\"speaker\": \"Guest\", \"command\": \"...\"}</code> Telegram approval requested for voice command <code>APPROVAL_RESOLVED</code> <code>{\"approved\": true, \"speaker\": \"Guest\"}</code> Approval resolved <code>EXEC_APPROVAL_REQUESTED</code> <code>{\"command\": \"git push\", \"request_id\": \"...\"}</code> OpenClaw exec approval requested <code>EXEC_APPROVAL_RESOLVED</code> <code>{\"approved\": true, \"request_id\": \"...\"}</code> Exec approval resolved"},{"location":"api/websocket/#other","title":"Other","text":"Event Data Description <code>SOUL_CHANGED</code> <code>{\"soul\": \"comedian\", \"name\": \"Comedian\"}</code> Personality switched <code>ERROR</code> <code>{\"message\": \"...\", \"source\": \"tts\"}</code> Error occurred"},{"location":"api/websocket/#client-commands","title":"Client Commands","text":"<p>Send JSON messages to the server:</p>"},{"location":"api/websocket/#ping","title":"Ping","text":"<pre><code>{\"type\": \"ping\"}\n</code></pre> <p>Response:</p> <pre><code>{\"event\": \"pong\"}\n</code></pre> <p>Use this to keep the connection alive or check connectivity.</p>"},{"location":"deployment/docker/","title":"Docker","text":""},{"location":"deployment/docker/#docker-run","title":"Docker Run","text":"<pre><code>docker build -t kiwi-voice .\n\ndocker run -d \\\n  --name kiwi-voice \\\n  --device /dev/snd \\\n  -v ./config.yaml:/app/config.yaml \\\n  -v ./.env:/app/.env \\\n  -v ./data:/app/data \\\n  -p 7789:7789 \\\n  kiwi-voice\n</code></pre> <p>Note</p> <p><code>--device /dev/snd</code> passes the sound devices to the container (Linux only). For audio output on macOS/Windows, use the Web Microphone instead.</p>"},{"location":"deployment/docker/#docker-compose","title":"Docker Compose","text":"<pre><code>version: \"3.8\"\nservices:\n  kiwi-voice:\n    build: .\n    restart: unless-stopped\n    devices:\n      - /dev/snd:/dev/snd\n    volumes:\n      - ./config.yaml:/app/config.yaml\n      - ./.env:/app/.env\n      - ./data:/app/data\n    ports:\n      - \"7789:7789\"\n    environment:\n      - KIWI_LANGUAGE=en\n      - KIWI_TTS_PROVIDER=kokoro\n</code></pre>"},{"location":"deployment/docker/#with-gpu-nvidia","title":"With GPU (NVIDIA)","text":"<pre><code>version: \"3.8\"\nservices:\n  kiwi-voice:\n    build: .\n    restart: unless-stopped\n    devices:\n      - /dev/snd:/dev/snd\n    volumes:\n      - ./config.yaml:/app/config.yaml\n      - ./.env:/app/.env\n      - ./data:/app/data\n    ports:\n      - \"7789:7789\"\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n    environment:\n      - KIWI_LANGUAGE=en\n      - KIWI_TTS_PROVIDER=kokoro\n</code></pre> <p>Requires the NVIDIA Container Toolkit.</p>"},{"location":"deployment/docker/#volumes","title":"Volumes","text":"Mount Purpose <code>config.yaml</code> Configuration <code>.env</code> Secrets <code>data/</code> Voice profiles, cached models"},{"location":"deployment/docker/#dashboard-access","title":"Dashboard Access","text":"<p>The dashboard is available at <code>http://localhost:7789</code> (or your server IP).</p>"},{"location":"deployment/gpu/","title":"GPU &amp; Apple Silicon","text":"<p>Kiwi Voice auto-detects GPU availability and falls back to CPU when no GPU is found.</p>"},{"location":"deployment/gpu/#nvidia-cuda","title":"NVIDIA CUDA","text":"<p>For GPU-accelerated STT and local TTS:</p> <pre><code>pip install torch torchaudio --index-url https://download.pytorch.org/whl/cu121\n</code></pre> <p>Verify:</p> <pre><code>python -c \"import torch; print(torch.cuda.is_available())\"\n</code></pre> <p>Configure in <code>config.yaml</code>:</p> <pre><code>stt:\n  device: \"cuda\"\n  compute_type: \"float16\"\n</code></pre> <p>Tip</p> <p>Even without a GPU, Kiwi works well \u2014 Faster Whisper runs on CPU with <code>int8</code> quantization, and Kokoro ONNX / Piper don't need a GPU at all.</p>"},{"location":"deployment/gpu/#apple-silicon-mlx","title":"Apple Silicon (MLX)","text":"<p>On M-series Macs, you can use Lightning Whisper MLX for ~10x faster STT:</p> <pre><code>pip install lightning-whisper-mlx\n</code></pre> <p>Configure:</p> <pre><code>stt:\n  engine: \"mlx-whisper\"\n  model: \"small\"          # or large, medium, etc.\n</code></pre> <p>MLX Whisper is auto-detected on Apple Silicon. On non-Apple hardware, it falls back to Faster Whisper.</p>"},{"location":"deployment/gpu/#cpu-only-setup","title":"CPU-Only Setup","text":"<p>Kiwi runs fully on CPU with these settings:</p> <pre><code>stt:\n  device: \"cpu\"\n  compute_type: \"int8\"\n  model: \"small\"          # Use small for better CPU performance\n\ntts:\n  provider: \"kokoro\"      # or piper \u2014 both are CPU-only\n</code></pre> <p>No CUDA, no GPU drivers needed. This is the lightest configuration.</p>"},{"location":"deployment/gpu/#vram-requirements","title":"VRAM Requirements","text":"<p>Approximate VRAM usage for GPU components:</p> Component Model VRAM Faster Whisper <code>small</code> ~1 GB Faster Whisper <code>large</code> ~3 GB Qwen3-TTS 0.6B ~2 GB Qwen3-TTS 1.7B ~4 GB pyannote (Speaker ID) \u2014 ~0.5 GB <p>Kokoro ONNX and Piper run on CPU and don't use VRAM.</p>"},{"location":"deployment/local/","title":"Local Setup","text":"<p>The simplest way to run Kiwi Voice \u2014 directly on your machine.</p>"},{"location":"deployment/local/#install","title":"Install","text":"<pre><code>git clone https://github.com/ekleziast/kiwi-voice.git\ncd kiwi-voice\n\npython -m venv venv\nsource venv/bin/activate          # Linux/macOS\n# source venv/Scripts/activate    # Windows/MSYS2\n\npip install -r requirements.txt\ncp .env.example .env              # Edit with your settings\n</code></pre>"},{"location":"deployment/local/#run","title":"Run","text":"Linux / macOSWindows (PowerShell)Windows (batch) <pre><code>python -m kiwi\n</code></pre> <pre><code>.\\start.ps1\n</code></pre> <pre><code>start.bat\n</code></pre> <p>Dashboard: http://localhost:7789</p>"},{"location":"deployment/local/#health-check","title":"Health Check","text":"<pre><code>curl http://localhost:7789/api/status\n</code></pre> <p>Expected:</p> <pre><code>{\"state\": \"LISTENING\", \"is_running\": true, ...}\n</code></pre>"},{"location":"deployment/local/#logs","title":"Logs","text":"<p>All logs go to the <code>logs/</code> directory:</p> File Content <code>logs/kiwi_startup.log</code> Startup sequence <code>logs/kiwi_crash_*.log</code> Crash reports <p>Runtime logs are printed to stdout. Use <code>KIWI_DEBUG=1</code> for verbose output.</p>"},{"location":"deployment/local/#troubleshooting","title":"Troubleshooting","text":"<p>No audio output: Check <code>audio.output_device</code> in <code>config.yaml</code>. List devices:</p> <pre><code>python -c \"import sounddevice; print(sounddevice.query_devices())\"\n</code></pre> <p>STT not recognizing speech: Check <code>stt.model</code> \u2014 <code>large</code> is most accurate. Also check microphone input level and the energy threshold in audio config.</p> <p>WebSocket connection failed: Make sure OpenClaw Gateway is running on the configured host and port (default: <code>127.0.0.1:18789</code>).</p> <p>Slow TTS: Try switching to Kokoro ONNX (fast, free, local) or ElevenLabs (fast, cloud).</p>"},{"location":"deployment/reverse-proxy/","title":"Reverse Proxy","text":"<p>Expose Kiwi Voice behind a reverse proxy with HTTPS. This is required for the Web Microphone to work on non-localhost origins (AudioWorklet requires a secure context).</p>"},{"location":"deployment/reverse-proxy/#nginx","title":"nginx","text":"<pre><code>server {\n    listen 443 ssl;\n    server_name kiwi.example.com;\n\n    ssl_certificate /etc/letsencrypt/live/kiwi.example.com/fullchain.pem;\n    ssl_certificate_key /etc/letsencrypt/live/kiwi.example.com/privkey.pem;\n\n    location / {\n        proxy_pass http://127.0.0.1:7789;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n    }\n}\n</code></pre> <p>WebSocket support</p> <p>The <code>Upgrade</code> and <code>Connection</code> headers are required for WebSocket connections (event stream and web audio).</p>"},{"location":"deployment/reverse-proxy/#caddy","title":"Caddy","text":"<pre><code>kiwi.example.com {\n    reverse_proxy localhost:7789\n}\n</code></pre> <p>Caddy handles SSL automatically via Let's Encrypt.</p>"},{"location":"deployment/reverse-proxy/#ssl-with-lets-encrypt","title":"SSL with Let's Encrypt","text":"<pre><code>sudo certbot --nginx -d kiwi.example.com\n</code></pre> <p>Or with Caddy \u2014 automatic, no extra steps needed.</p>"},{"location":"deployment/systemd/","title":"systemd Service (Linux)","text":"<p>Run Kiwi Voice as a system service that starts on boot and auto-restarts on failure.</p>"},{"location":"deployment/systemd/#create-the-service","title":"Create the Service","text":"<p>Create <code>/etc/systemd/system/kiwi-voice.service</code>:</p> <pre><code>[Unit]\nDescription=Kiwi Voice Assistant\nAfter=network.target sound.target\n\n[Service]\nType=simple\nUser=kiwi\nWorkingDirectory=/opt/kiwi-voice\nExecStart=/opt/kiwi-voice/venv/bin/python -m kiwi\nRestart=on-failure\nRestartSec=5\nEnvironment=KIWI_LANGUAGE=en\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"deployment/systemd/#enable-and-start","title":"Enable and Start","text":"<pre><code>sudo systemctl daemon-reload\nsudo systemctl enable kiwi-voice\nsudo systemctl start kiwi-voice\n</code></pre>"},{"location":"deployment/systemd/#view-logs","title":"View Logs","text":"<pre><code>sudo journalctl -u kiwi-voice -f\n</code></pre>"},{"location":"deployment/systemd/#commands","title":"Commands","text":"Command Action <code>systemctl start kiwi-voice</code> Start the service <code>systemctl stop kiwi-voice</code> Stop the service <code>systemctl restart kiwi-voice</code> Restart <code>systemctl status kiwi-voice</code> Check status <code>systemctl enable kiwi-voice</code> Enable on boot <code>systemctl disable kiwi-voice</code> Disable on boot"},{"location":"deployment/systemd/#audio-access","title":"Audio Access","text":"<p>The service user needs access to audio devices. Add the user to the <code>audio</code> group:</p> <pre><code>sudo usermod -aG audio kiwi\n</code></pre>"},{"location":"development/architecture/","title":"Architecture","text":""},{"location":"development/architecture/#audio-pipeline","title":"Audio Pipeline","text":"<pre><code>Mic (24kHz) / Browser WebSocket\n  \u2192 Audio Callback (energy detection + Silero VAD)\n  \u2192 Audio Queue\n  \u2192 KiwiListener._record_loop()\n  \u2192 Faster Whisper STT (or MLX Whisper on Apple Silicon)\n  \u2192 Wake Word Detection (\"kiwi\" \u2014 text fuzzy match or ML pre-detection)\n  \u2192 Speaker ID (pyannote embedding \u2192 cosine similarity)\n  \u2192 Priority Gate (OWNER &gt; FRIEND &gt; GUEST &gt; BLOCKED)\n  \u2192 Voice Security (DangerousCommandDetector \u2192 Telegram approval)\n  \u2192 OpenClaw Gateway (WebSocket v3: chat.send \u2192 delta/final events)\n  \u2192 LLM response stream (delta \u2192 sentence chunking)\n  \u2192 Streaming TTS (Kokoro / Piper / Qwen3 / ElevenLabs)\n  \u2192 Speaker Output (with barge-in detection)\n  \u2192 Loop back to listening\n</code></pre>"},{"location":"development/architecture/#key-modules","title":"Key Modules","text":"Module File Purpose Service <code>kiwi/service.py</code> Main orchestrator, lifecycle management Listener <code>kiwi/listener.py</code> Microphone capture, VAD, STT, wake word OpenClaw WS <code>kiwi/openclaw_ws.py</code> WebSocket client to OpenClaw Gateway Speaker Manager <code>kiwi/speaker_manager.py</code> Voiceprint storage, identification, priority Voice Security <code>kiwi/voice_security.py</code> Dangerous command detection, Telegram approval Soul Manager <code>kiwi/soul_manager.py</code> Personality loading and switching i18n <code>kiwi/i18n.py</code> Internationalization (<code>t()</code> function) Event Bus <code>kiwi/event_bus.py</code> Internal pub/sub event system API Server <code>kiwi/api/server.py</code> REST API + WebSocket events, aiohttp TTS Providers <code>kiwi/tts/</code> ElevenLabs, Kokoro, Piper, Qwen3"},{"location":"development/architecture/#mixins","title":"Mixins","text":"<p>The main service class uses mixins to separate concerns:</p> Mixin File Responsibility <code>LLMCallbacks</code> <code>kiwi/mixins/llm_callbacks.py</code> LLM token/completion/exec approval handlers <code>DialoguePipeline</code> <code>kiwi/mixins/dialogue_pipeline.py</code> Dialogue stages including approval checks"},{"location":"development/architecture/#event-bus","title":"Event Bus","text":"<p>Kiwi uses an internal event bus (<code>kiwi/event_bus.py</code>) for decoupled communication between modules:</p> <pre><code>from kiwi.event_bus import EventBus\n\nbus = EventBus()\nbus.subscribe(\"WAKE_WORD_DETECTED\", handler)\nbus.emit(\"WAKE_WORD_DETECTED\", {\"text\": \"kiwi\"})\n</code></pre> <p>Events are also forwarded to the WebSocket API (<code>/api/events</code>) for the dashboard and external integrations.</p> <p>Key events: <code>STATE_CHANGED</code>, <code>WAKE_WORD_DETECTED</code>, <code>SPEECH_RECOGNIZED</code>, <code>SPEAKER_IDENTIFIED</code>, <code>TTS_STARTED</code>, <code>TTS_FINISHED</code>, <code>LLM_TOKEN</code>, <code>LLM_COMPLETE</code>, <code>EXEC_APPROVAL_REQUESTED</code>, <code>EXEC_APPROVAL_RESOLVED</code>, <code>SOUL_CHANGED</code>, <code>ERROR</code>.</p>"},{"location":"development/architecture/#openclaw-protocol","title":"OpenClaw Protocol","text":"<p>Kiwi communicates with OpenClaw via WebSocket Gateway v3:</p> <ol> <li>Connect to <code>ws://127.0.0.1:18789</code></li> <li>Send <code>chat.send</code> with user message</li> <li>Receive <code>delta</code> events (streaming tokens) and <code>final</code> event (complete response)</li> <li>Subscribe to <code>exec.approval.requested</code> for shell command approvals</li> <li>Send <code>exec.approval.resolve</code> with approve/deny decision</li> </ol>"},{"location":"development/architecture/#threading-model","title":"Threading Model","text":"<ul> <li>Main thread: Service lifecycle, signal handling</li> <li>Audio thread: Microphone callback (daemon)</li> <li>Record loop: STT + wake word + command processing (daemon)</li> <li>TTS thread: Audio output (daemon)</li> <li>API thread: aiohttp server in separate event loop (daemon)</li> <li>WebSocket thread: OpenClaw Gateway connection (daemon)</li> </ul> <p>All background threads are daemon threads with crash protection (try/except + sleep + continue in loops). Shared resources are guarded by <code>threading.Lock</code>.</p>"},{"location":"development/code-patterns/","title":"Code Patterns","text":"<p>Conventions and patterns used throughout the Kiwi Voice codebase.</p>"},{"location":"development/code-patterns/#logging","title":"Logging","text":"<p>Always use <code>kiwi_log()</code> \u2014 never bare <code>print()</code>:</p> <pre><code>from kiwi.utils import kiwi_log\n\nkiwi_log(\"TAG\", \"message\", level=\"INFO\")\n# \u2192 [14:08:25.342] [INFO] [TAG] message\n</code></pre>"},{"location":"development/code-patterns/#project-root-paths","title":"Project Root Paths","text":"<p>Use <code>PROJECT_ROOT</code> for paths to project-level assets:</p> <pre><code>from kiwi import PROJECT_ROOT\nimport os\n\npath = os.path.join(PROJECT_ROOT, 'sounds', 'startup.mp3')\n</code></pre>"},{"location":"development/code-patterns/#i18n-strings","title":"i18n Strings","text":"<p>User-facing strings always go through <code>t()</code>. Developer-facing log messages do not:</p> <pre><code>from kiwi.i18n import t\n\n# User-facing \u2014 use t()\nself._speak(t(\"responses.greeting\"))\nself._speak(t(\"responses.heard\", command=cmd))\n\n# Developer-facing \u2014 plain string\nkiwi_log(\"TAG\", \"Internal log message\", level=\"INFO\")\n</code></pre> <p>Module-level constants are kept as fallback defaults. Instance attributes override them from i18n at init time:</p> <pre><code>self.wake_word = t(\"wake_word.keyword\") or WAKE_WORD\nself.hallucination_phrases = set(t(\"hallucinations.phrases\") or HALLUCINATION_PHRASES)\n</code></pre>"},{"location":"development/code-patterns/#optional-module-loading","title":"Optional Module Loading","text":"<p>Modules with optional dependencies use try/except with availability flags:</p> <pre><code>try:\n    from kiwi.speaker_manager import SpeakerManager\n    SPEAKER_MANAGER_AVAILABLE = True\nexcept ImportError:\n    SPEAKER_MANAGER_AVAILABLE = False\n</code></pre> <p>Then check the flag before using:</p> <pre><code>if SPEAKER_MANAGER_AVAILABLE:\n    self.speaker_manager = SpeakerManager()\n</code></pre>"},{"location":"development/code-patterns/#gpu-auto-detection","title":"GPU Auto-Detection","text":"<p>CUDA is used when available, with automatic CPU fallback:</p> <pre><code>import torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n</code></pre>"},{"location":"development/code-patterns/#threading","title":"Threading","text":"<p>All background work uses daemon threads with crash protection:</p> <pre><code>import threading\n\nthread = threading.Thread(target=self._worker, daemon=True)\nthread.start()\n\ndef _worker(self):\n    while self._running:\n        try:\n            # work\n            pass\n        except Exception as e:\n            kiwi_log(\"WORKER\", f\"Error: {e}\", level=\"ERROR\")\n            time.sleep(1)\n            continue\n</code></pre> <p>Shared resources are guarded by <code>threading.Lock</code>:</p> <pre><code>self._lock = threading.Lock()\n\nwith self._lock:\n    self._cache[key] = value\n</code></pre>"},{"location":"development/code-patterns/#windows-utf-8","title":"Windows UTF-8","text":"<p>Console codepage is set for Unicode output:</p> <pre><code>import ctypes\nctypes.windll.kernel32.SetConsoleCP(65001)\n</code></pre>"},{"location":"development/code-patterns/#tests","title":"Tests","text":"<p>Run tests:</p> <pre><code>pytest tests/ -v\n</code></pre> <p>Smoke tests verify module imports and basic config loading:</p> <pre><code>pytest tests/test_smoke.py\n</code></pre>"},{"location":"development/contributing/","title":"Contributing","text":""},{"location":"development/contributing/#setup","title":"Setup","text":"<pre><code>git clone https://github.com/ekleziast/kiwi-voice.git\ncd kiwi-voice\npython -m venv venv\nsource venv/bin/activate\npip install -r requirements.txt\n</code></pre>"},{"location":"development/contributing/#code-style","title":"Code Style","text":"<ul> <li>Linter: Ruff for linting and formatting</li> <li>Type checking: mypy (with <code>--ignore-missing-imports</code>)</li> <li>Logging: Always <code>kiwi_log(\"TAG\", \"message\")</code> \u2014 never <code>print()</code></li> <li>Paths: Use <code>PROJECT_ROOT</code> from <code>kiwi</code> package</li> <li>Imports: Optional modules with try/except + <code>*_AVAILABLE</code> flags</li> <li>Threads: Daemon threads with crash protection</li> <li>Comments and docstrings: English</li> <li>User-facing strings: Externalized via <code>t()</code> into locale YAML files</li> </ul>"},{"location":"development/contributing/#running-checks","title":"Running Checks","text":"<pre><code># Lint\nruff check kiwi/ tests/\n\n# Format check\nruff format --check kiwi/ tests/\n\n# Type check\nmypy kiwi/ --ignore-missing-imports --no-strict-optional\n\n# Tests\npytest tests/ -v --tb=short\n</code></pre> <p>CI runs these checks on every push and PR via GitHub Actions.</p>"},{"location":"development/contributing/#project-structure","title":"Project Structure","text":"<pre><code>kiwi-voice/\n\u251c\u2500\u2500 kiwi/                    # Main package\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 __main__.py          # Entry point\n\u2502   \u251c\u2500\u2500 service.py           # Main service orchestrator\n\u2502   \u251c\u2500\u2500 listener.py          # Audio capture, STT, wake word\n\u2502   \u251c\u2500\u2500 openclaw_ws.py       # WebSocket client\n\u2502   \u251c\u2500\u2500 speaker_manager.py   # Speaker identification\n\u2502   \u251c\u2500\u2500 voice_security.py    # Dangerous command detection\n\u2502   \u251c\u2500\u2500 soul_manager.py      # Personality system\n\u2502   \u251c\u2500\u2500 i18n.py              # Internationalization\n\u2502   \u251c\u2500\u2500 event_bus.py         # Internal event system\n\u2502   \u251c\u2500\u2500 utils.py             # Logging, helpers\n\u2502   \u251c\u2500\u2500 tts/                 # TTS providers\n\u2502   \u251c\u2500\u2500 mixins/              # Service mixins\n\u2502   \u251c\u2500\u2500 api/                 # REST API server\n\u2502   \u251c\u2500\u2500 web/                 # Dashboard (HTML/CSS/JS)\n\u2502   \u251c\u2500\u2500 locales/             # Language YAML files\n\u2502   \u2514\u2500\u2500 souls/               # Personality markdown files\n\u251c\u2500\u2500 custom_components/       # Home Assistant integration\n\u251c\u2500\u2500 tests/                   # Test suite\n\u251c\u2500\u2500 docs/                    # Documentation (MkDocs)\n\u251c\u2500\u2500 config.yaml              # Configuration\n\u251c\u2500\u2500 .env.example             # Environment template\n\u251c\u2500\u2500 requirements.txt         # Python dependencies\n\u2514\u2500\u2500 mkdocs.yml               # Docs config\n</code></pre>"},{"location":"development/contributing/#adding-a-language","title":"Adding a Language","text":"<p>See Multi-Language Support.</p>"},{"location":"development/contributing/#adding-a-tts-provider","title":"Adding a TTS Provider","text":"<ol> <li>Create a new file in <code>kiwi/tts/</code> implementing the provider interface</li> <li>Register it in the TTS factory</li> <li>Add config section in <code>config.yaml</code></li> <li>Document in <code>docs/features/tts-providers.md</code></li> </ol>"},{"location":"development/contributing/#adding-a-soul","title":"Adding a Soul","text":"<p>Create a markdown file in <code>kiwi/souls/</code> \u2014 see Personalities.</p>"},{"location":"features/home-assistant/","title":"Home Assistant Integration","text":"<p>Kiwi Voice integrates bidirectionally with Home Assistant:</p> <ul> <li>HA \u2192 Kiwi: Control Kiwi from the HA dashboard (switches, buttons, sensors)</li> <li>Kiwi \u2192 HA: Control your smart home by voice through Kiwi via the Conversation API</li> </ul>"},{"location":"features/home-assistant/#installation","title":"Installation","text":"<ol> <li> <p>Copy the custom component to your HA installation:</p> <pre><code>cp -r custom_components/kiwi_voice/ /path/to/homeassistant/custom_components/kiwi_voice/\n</code></pre> </li> <li> <p>Restart Home Assistant</p> </li> <li> <p>Add the integration via UI: Settings \u2192 Integrations \u2192 Add Integration \u2192 Kiwi Voice</p> </li> <li> <p>Enter the Kiwi Voice API URL (e.g., <code>http://192.168.1.100:7789</code>)</p> </li> </ol> <p>The integration auto-discovers Kiwi Voice on your network.</p>"},{"location":"features/home-assistant/#entities","title":"Entities","text":"Entity Type Description State Sensor Current Kiwi state (idle, listening, thinking, speaking) Language Sensor Active language code HA Connected Binary Sensor Whether Kiwi is connected to HA Speakers Sensor Number of known speaker profiles Uptime Sensor Service uptime Listening Switch Enable/disable microphone listening Stop Button Stop current TTS playback Reset Context Button Clear conversation context TTS Test Button Speak a test phrase TTS Platform Use Kiwi as a TTS platform in HA"},{"location":"features/home-assistant/#voice-control","title":"Voice Control","text":"<p>Say a Home Assistant command through Kiwi:</p> <p>\"Kiwi, turn on the lights in the bedroom\"</p> <p>The command is routed to the Home Assistant Conversation API and the response is spoken back by Kiwi.</p>"},{"location":"features/home-assistant/#configuration","title":"Configuration","text":""},{"location":"features/home-assistant/#kiwi-side","title":"Kiwi Side","text":"<pre><code># config.yaml\nhomeassistant:\n  enabled: true\n  url: \"http://homeassistant.local:8123\"\n  token: \"\"    # Long-Lived Access Token from HA\n</code></pre> <p>To create a Long-Lived Access Token:</p> <ol> <li>Go to your HA profile page</li> <li>Scroll to Long-Lived Access Tokens</li> <li>Click Create Token</li> <li>Copy the token to <code>config.yaml</code></li> </ol>"},{"location":"features/home-assistant/#ha-side","title":"HA Side","text":"<p>The custom component connects to Kiwi's REST API and WebSocket for real-time state updates. No additional HA configuration is needed beyond adding the integration.</p>"},{"location":"features/home-assistant/#rest-api","title":"REST API","text":"<p>Kiwi also exposes a Home Assistant endpoint:</p> <pre><code># Check HA connection status\ncurl http://localhost:7789/api/homeassistant/status\n\n# Send a voice command to HA\ncurl -X POST http://localhost:7789/api/homeassistant/command \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"text\": \"turn on bedroom lights\", \"language\": \"en\"}'\n</code></pre>"},{"location":"features/multilanguage/","title":"Multi-Language Support","text":"<p>Kiwi ships with 15 languages out of the box. All user-facing strings, voice commands, wake word variants, hallucination filters, and security patterns are fully localized.</p>"},{"location":"features/multilanguage/#available-languages","title":"Available Languages","text":"Code Language Code Language <code>en</code> English <code>tr</code> Turkish <code>ru</code> Russian <code>pl</code> Polish <code>es</code> Spanish <code>zh</code> Chinese <code>pt</code> Portuguese <code>ja</code> Japanese <code>fr</code> French <code>ko</code> Korean <code>it</code> Italian <code>hi</code> Hindi <code>de</code> German <code>ar</code> Arabic <code>id</code> Indonesian"},{"location":"features/multilanguage/#switching-language","title":"Switching Language","text":""},{"location":"features/multilanguage/#via-configyaml","title":"Via config.yaml","text":"<pre><code>language: \"en\"\n</code></pre>"},{"location":"features/multilanguage/#via-environment-variable","title":"Via environment variable","text":"<pre><code>KIWI_LANGUAGE=es python -m kiwi\n</code></pre>"},{"location":"features/multilanguage/#via-rest-api-runtime","title":"Via REST API (runtime)","text":"<pre><code>curl -X POST http://localhost:7789/api/language \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"language\": \"fr\"}'\n</code></pre>"},{"location":"features/multilanguage/#via-dashboard","title":"Via Dashboard","text":"<p>Use the Language panel dropdown and click Apply.</p>"},{"location":"features/multilanguage/#what-gets-localized","title":"What Gets Localized","text":"<p>Each locale YAML file (<code>kiwi/locales/{code}.yaml</code>) contains 19 sections:</p> Section Content <code>system</code> Voice system prompt for the LLM <code>responses</code> ~37 spoken user-facing strings <code>status</code> Long-task status announcements <code>security</code> Warning messages, Telegram button labels <code>speakers</code> Speaker identification responses <code>speaker_access</code> Access control messages <code>wake_word</code> Keyword, typos, fuzzy blacklist <code>hallucinations</code> Whisper hallucination phrases/patterns <code>text_processing</code> Abbreviations, incomplete/complete patterns, emotion keywords <code>security_patterns</code> Dangerous command regexes <code>dangerous_commands</code> Example dangerous commands <code>owner_commands</code> Voice control command phrases <code>owner_control_patterns</code> Owner command regexes <code>name_patterns</code> Name extraction regexes and filter words <code>commands</code> Command keywords (stop, calibrate, approval, etc.) <code>cli_errors</code> CLI error messages <code>ws_errors</code> WebSocket error messages <code>tool_activity</code> Tool status descriptions <code>tool_errors</code> Tool error messages"},{"location":"features/multilanguage/#adding-a-new-language","title":"Adding a New Language","text":"<ol> <li> <p>Copy the English locale as a template:</p> <pre><code>cp kiwi/locales/en.yaml kiwi/locales/sv.yaml\n</code></pre> </li> <li> <p>Translate all string values. Preserve keys and <code>{placeholder}</code> tokens:</p> <pre><code>responses:\n  greeting: \"Hej! Jag \u00e4r Kiwi...\"\n  heard: \"H\u00f6rde: {command}\"\n</code></pre> </li> <li> <p>Adapt language-specific sections that require linguistic knowledge:</p> <ul> <li><code>wake_word.typos</code> \u2014 phonetic variants in the target language</li> <li><code>hallucinations.phrases</code> \u2014 common Whisper hallucinations for that language</li> <li><code>text_processing</code> \u2014 abbreviations and patterns</li> <li><code>security_patterns</code> \u2014 dangerous command regexes in the target language</li> </ul> </li> <li> <p>Set the language:</p> <pre><code>language: \"sv\"\n</code></pre> </li> </ol>"},{"location":"features/multilanguage/#i18n-api","title":"i18n API","text":"<p>Internally, Kiwi uses the <code>t()</code> function from <code>kiwi/i18n.py</code>:</p> <pre><code>from kiwi.i18n import setup, t\n\nsetup(\"en\", fallback=\"ru\")       # Initialize\nt(\"responses.greeting\")           # \u2192 \"Hello! I'm Kiwi...\"\nt(\"responses.heard\", command=cmd) # \u2192 \"Heard: {command}\" with placeholder\nt(\"hallucinations.phrases\")       # \u2192 returns a list\n</code></pre> <ul> <li>Dot-notation key resolution</li> <li>Automatic fallback to the fallback locale if a key is missing</li> <li>Returns lists and dicts as-is for non-string values</li> </ul>"},{"location":"features/souls/","title":"Personalities (Soul System)","text":"<p>Kiwi supports dynamic personality switching through markdown-based \"souls\" \u2014 each soul defines a system prompt overlay that shapes how Kiwi responds.</p>"},{"location":"features/souls/#built-in-souls","title":"Built-in Souls","text":"Soul Style NSFW Mindful Companion Calm, supportive, thoughtful No Storyteller Narrative, descriptive, immersive No Comedian Witty, sarcastic, humorous No Hype Person Energetic, motivational, enthusiastic No Siren Flirty, provocative Yes (18+)"},{"location":"features/souls/#how-it-works","title":"How it Works","text":"<p>Souls are markdown files in <code>kiwi/souls/</code>. The <code>SoulManager</code> loads them at startup and composes the final system prompt:</p> <pre><code>Base system prompt (SOUL.md) + Soul personality overlay = Final prompt\n</code></pre> <p>The base prompt defines Kiwi's core identity and capabilities. The soul overlay adds personality traits on top.</p>"},{"location":"features/souls/#switching-souls","title":"Switching Souls","text":""},{"location":"features/souls/#by-voice","title":"By Voice","text":"<p>\"Kiwi, be a comedian\" \"Kiwi, switch to storyteller\" \"Kiwi, default mode\" (resets to Mindful Companion)</p>"},{"location":"features/souls/#via-dashboard","title":"Via Dashboard","text":"<p>Click any personality card in the dashboard carousel to activate it.</p>"},{"location":"features/souls/#via-rest-api","title":"Via REST API","text":"<pre><code>curl -X POST http://localhost:7789/api/soul \\\n  -H \"Content-Type: application/json\" \\\n  -d '{\"soul\": \"comedian\"}'\n</code></pre>"},{"location":"features/souls/#query-current-soul","title":"Query Current Soul","text":"<pre><code>curl http://localhost:7789/api/soul/current\n</code></pre>"},{"location":"features/souls/#nsfw-routing","title":"NSFW Routing","text":"<p>The Siren soul (and any custom NSFW soul) is fully isolated:</p> <ul> <li>Routes to a separate OpenClaw agent with its own LLM model and session</li> <li>The session is switched via <code>openclaw_ws.switch_session()</code></li> <li>When switching back to a non-NSFW soul, the previous session is restored</li> </ul> <p>Configure the NSFW backend:</p> <pre><code>souls:\n  default: \"mindful-companion\"\n  nsfw:\n    model: \"openrouter/mistralai/mistral-7b-instruct\"\n    session: \"kiwi-nsfw\"\n</code></pre>"},{"location":"features/souls/#creating-custom-souls","title":"Creating Custom Souls","text":"<ol> <li> <p>Create a markdown file in <code>kiwi/souls/</code>:</p> <pre><code>---\nname: Philosopher\ndescription: Deep thinker who ponders existence\nmodel: ~\nnsfw: false\n---\n\n## Identity\n\nYou are a contemplative philosopher. You speak in measured, thoughtful sentences.\nYou reference ancient wisdom and draw connections to modern life.\n\n## Style\n\n- Speak slowly and deliberately\n- Ask thought-provoking questions\n- Reference philosophy and literature\n</code></pre> </li> <li> <p>Restart Kiwi \u2014 the new soul appears in the dashboard and is available via voice/API.</p> </li> </ol> <p>The <code>model</code> field is informational. Set <code>nsfw: true</code> to route through the isolated NSFW session.</p>"},{"location":"features/speaker-id/","title":"Speaker Identification","text":"<p>Kiwi identifies speakers by their voiceprint using pyannote neural embeddings. This enables personalized responses and voice-gated security.</p>"},{"location":"features/speaker-id/#how-it-works","title":"How it Works","text":"<ol> <li>When someone speaks, Kiwi extracts a 192-dimensional embedding vector from the audio</li> <li>This vector is compared against stored voiceprints using cosine similarity</li> <li>If the similarity exceeds the threshold, the speaker is identified</li> <li>If no match is found, Kiwi treats the speaker as a new guest</li> </ol>"},{"location":"features/speaker-id/#priority-system","title":"Priority System","text":"<p>Every speaker is assigned a priority level:</p> Priority Level Access <code>OWNER</code> (0) Highest Full access. Cannot be blocked. <code>FRIEND</code> (1) High Dangerous commands need Telegram approval <code>GUEST</code> (2) Default All sensitive commands need approval <code>BLOCKED</code> (99) None Completely ignored <p>There's also a special <code>SELF</code> (-1) priority used internally for TTS echo filtering \u2014 Kiwi ignores its own voice output.</p>"},{"location":"features/speaker-id/#voice-commands","title":"Voice Commands","text":"Command Action \"Kiwi, remember my voice\" Register as owner (first registration only) \"Kiwi, this is my friend [name]\" Add the current speaker as a friend \"Kiwi, block them\" Block the last unrecognized speaker \"Kiwi, who is speaking?\" Identify the current speaker \"Kiwi, what voices do you know?\" List all known voiceprints <p>Note</p> <p>Voice commands are language-dependent. Set <code>language</code> in <code>config.yaml</code> to match your locale. See <code>kiwi/locales/*.yaml</code> for per-language command variants.</p>"},{"location":"features/speaker-id/#auto-learning","title":"Auto-Learning","text":"<p>When Kiwi encounters an unknown voice, it automatically stores the voiceprint as a guest. On subsequent interactions, the speaker is recognized without any manual registration.</p> <p>The owner can then upgrade a guest to a friend or block them via voice commands.</p>"},{"location":"features/speaker-id/#voiceprint-storage","title":"Voiceprint Storage","text":"<p>Voiceprints are stored as JSON files in the <code>voice_profiles/</code> directory:</p> <pre><code>voice_profiles/\n\u251c\u2500\u2500 owner.json          # Owner voiceprint + metadata\n\u251c\u2500\u2500 friend_alice.json   # Friend profile\n\u2514\u2500\u2500 guest_001.json      # Auto-learned guest\n</code></pre> <p>Each file contains the embedding vector, speaker name, priority level, sample count, and last seen timestamp.</p> <p>Reset all profiles:</p> <pre><code>rm voice_profiles/*.json\n# Restart Kiwi and re-register your voice\n</code></pre>"},{"location":"features/speaker-id/#configuration","title":"Configuration","text":"<pre><code>speaker_priority:\n  owner:\n    name: \"Owner\"    # Your display name\n</code></pre>"},{"location":"features/streaming-tts/","title":"Streaming TTS &amp; Barge-In","text":""},{"location":"features/streaming-tts/#streaming-tts","title":"Streaming TTS","text":"<p>Kiwi doesn't wait for the full LLM response before speaking. Instead, it uses sentence-aware chunking to start TTS as soon as the first complete sentence arrives.</p>"},{"location":"features/streaming-tts/#how-it-works","title":"How it Works","text":"<ol> <li>LLM streams response tokens via WebSocket (<code>delta</code> events)</li> <li>Kiwi's sentence chunker accumulates tokens until it detects a sentence boundary (<code>.</code>, <code>!</code>, <code>?</code>, or other punctuation)</li> <li>The completed sentence is immediately sent to the TTS provider</li> <li>Audio playback begins while the LLM is still generating the next sentence</li> <li>Subsequent sentences are queued and played sequentially</li> </ol> <p>This reduces perceived latency significantly \u2014 the user hears the first sentence within ~1 second of the LLM starting to respond, regardless of total response length.</p>"},{"location":"features/streaming-tts/#sentence-chunking","title":"Sentence Chunking","text":"<p>The chunker is language-aware and handles:</p> <ul> <li>Standard punctuation (<code>.</code>, <code>!</code>, <code>?</code>)</li> <li>Abbreviations that shouldn't trigger a split (e.g., \"Dr.\", \"U.S.A.\")</li> <li>Minimum chunk length to avoid tiny audio fragments</li> <li>Maximum chunk length to prevent overly long sentences</li> </ul>"},{"location":"features/streaming-tts/#barge-in","title":"Barge-In","text":"<p>Barge-in lets you interrupt Kiwi mid-sentence by speaking over it.</p>"},{"location":"features/streaming-tts/#how-it-works_1","title":"How it Works","text":"<ol> <li>While Kiwi is speaking (TTS playing), the microphone remains passively active</li> <li>If the energy level or VAD detects speech during playback, Kiwi:<ul> <li>Immediately stops TTS playback</li> <li>Clears the audio queue</li> <li>Switches back to active listening mode</li> </ul> </li> <li>Your new command is processed normally</li> </ol> <p>This creates a natural conversational flow \u2014 you don't have to wait for Kiwi to finish a long response before giving a new command.</p> <p>Note</p> <p>Barge-in sensitivity is tied to the VAD (Voice Activity Detection) settings. If it triggers too easily, adjust the energy threshold in the audio configuration.</p>"},{"location":"features/tts-providers/","title":"TTS Providers","text":"<p>Kiwi supports five text-to-speech providers. Switch between them in <code>config.yaml</code> or via environment variable.</p>"},{"location":"features/tts-providers/#comparison","title":"Comparison","text":"Provider Quality Latency Cost Local GPU Languages ElevenLabs Excellent ~0.3s ~$0.30/1K chars No 29 Qwen3-TTS (local) Excellent ~1\u20133s Free Yes (CUDA) Many Qwen3-TTS (RunPod) Excellent ~2\u20135s ~$0.0003/sec No Many Kokoro ONNX High &lt;0.5s Free No 8 Piper Good &lt;0.5s Free No 30+"},{"location":"features/tts-providers/#kokoro-onnx","title":"Kokoro ONNX","text":"<p>FREE RECOMMENDED</p> <p>Fully local TTS with 14 voices at 24kHz. Models auto-download on first use (~340MB). No GPU needed.</p> <pre><code>tts:\n  provider: \"kokoro\"\n  kokoro:\n    voice: \"af_heart\"\n    speed: 1.0\n</code></pre> <p>Supports English, Japanese, Chinese, Korean, and several European languages. Russian is not yet supported \u2014 use Piper or ElevenLabs for Russian.</p>"},{"location":"features/tts-providers/#piper","title":"Piper","text":"<p>FREE</p> <p>Fast local TTS using ONNX models. Wide language support including Russian.</p> <pre><code>tts:\n  provider: \"piper\"\n  piper:\n    model: \"en_US-lessac-medium\"\n</code></pre> <p>Models are downloaded automatically. See the Piper voices list for available models.</p>"},{"location":"features/tts-providers/#elevenlabs","title":"ElevenLabs","text":"<p>PAID</p> <p>Cloud-based TTS with the lowest latency and highest voice quality. Requires an API key.</p> <pre><code>tts:\n  provider: \"elevenlabs\"\n  elevenlabs:\n    voice_id: \"aEO01A4wXwd1O8GPgGlF\"\n    model_id: \"eleven_multilingual_v2\"\n    stability: 0.45\n    similarity_boost: 0.75\n    speed: 1.0\n</code></pre> <pre><code># .env\nKIWI_ELEVENLABS_API_KEY=sk-...\n</code></pre>"},{"location":"features/tts-providers/#qwen3-tts-local","title":"Qwen3-TTS (Local)","text":"<p>FREE GPU REQUIRED</p> <p>High-quality local TTS using the Qwen3-TTS model. Requires a CUDA GPU with sufficient VRAM.</p> <pre><code>tts:\n  provider: \"qwen3\"\n  qwen3:\n    backend: \"local\"\n</code></pre>"},{"location":"features/tts-providers/#qwen3-tts-runpod","title":"Qwen3-TTS (RunPod)","text":"<p>PAY-PER-USE</p> <p>Same Qwen3-TTS quality, but running on RunPod serverless GPUs. No local GPU needed.</p> <pre><code>tts:\n  provider: \"qwen3\"\n  qwen3:\n    backend: \"runpod\"\n</code></pre> <pre><code># .env\nRUNPOD_API_KEY=...\nRUNPOD_TTS_ENDPOINT_ID=...\n</code></pre>"},{"location":"features/tts-providers/#switching-providers","title":"Switching Providers","text":""},{"location":"features/tts-providers/#via-configyaml","title":"Via config.yaml","text":"<pre><code>tts:\n  provider: \"kokoro\"   # kokoro, piper, qwen3, elevenlabs\n</code></pre>"},{"location":"features/tts-providers/#via-environment-variable","title":"Via environment variable","text":"<pre><code>KIWI_TTS_PROVIDER=kokoro python -m kiwi\n</code></pre>"},{"location":"features/tts-providers/#via-rest-api","title":"Via REST API","text":"<p>Test TTS without changing config:</p> <pre><code>curl -X POST http://localhost:7789/api/tts/test \\\n  -d '{\"text\": \"Hello, I am Kiwi!\"}'\n</code></pre>"},{"location":"features/voice-security/","title":"Voice Security","text":"<p>Kiwi implements two-layer security that catches dangerous commands both before and after they reach the LLM.</p>"},{"location":"features/voice-security/#layer-1-pre-llm-filter-kiwi","title":"Layer 1: Pre-LLM Filter (Kiwi)","text":"<p>Before any command is sent to the LLM, Kiwi's <code>DangerousCommandDetector</code> analyzes it:</p> <ol> <li>Speaker identification \u2014 determines who is speaking via voiceprint</li> <li>Command classification \u2014 regex-based patterns classify commands into severity levels:</li> </ol> Level Examples Owner Friend Guest <code>SAFE</code> \"What time is it?\", \"Tell me a joke\" Pass Pass Pass <code>WARNING</code> \"Open the garage door\" Pass Telegram approval Telegram approval <code>DANGEROUS</code> \"Delete all files\", \"Format disk\" Pass Telegram approval Blocked <code>CRITICAL</code> \"rm -rf /\", \"Drop database\" Pass Blocked Blocked <ol> <li>Telegram approval \u2014 for non-owner speakers, dangerous commands trigger a Telegram message to the owner with Approve/Deny buttons</li> </ol>"},{"location":"features/voice-security/#security-patterns","title":"Security Patterns","text":"<p>Dangerous command patterns are defined per-language in locale files (<code>kiwi/locales/*.yaml</code> \u2192 <code>security_patterns</code> section), plus universal patterns that work across all languages.</p> <p>Examples: <pre><code>security_patterns:\n  dangerous:\n    - \"(?i)(delete|remove|destroy)\\\\s+(all|every)\"\n    - \"(?i)(format|wipe)\\\\s+(disk|drive|partition)\"\n  critical:\n    - \"(?i)rm\\\\s+-rf\"\n    - \"(?i)drop\\\\s+(database|table)\"\n  universal:\n    - \"(?i)(sudo|chmod|chown)\\\\s+\"\n    - \"(?i)(shutdown|reboot|poweroff)\"\n</code></pre></p>"},{"location":"features/voice-security/#layer-2-post-llm-filter-openclaw","title":"Layer 2: Post-LLM Filter (OpenClaw)","text":"<p>Even if a command passes the pre-filter, the actual shell execution still requires approval:</p> <ol> <li>User gives a voice command \u2192 passes pre-filter \u2192 reaches the LLM</li> <li>LLM decides to run a shell command via the <code>exec</code> tool</li> <li>OpenClaw Gateway broadcasts an <code>exec.approval.requested</code> event</li> <li>Kiwi receives the event and:<ul> <li>Announces the command to the owner via voice: \"The agent wants to run: git push origin main\"</li> <li>Sends a Telegram message with Approve/Deny buttons</li> </ul> </li> <li>Owner responds by voice (\"allow\" or \"deny\") or via Telegram</li> <li>Kiwi sends the decision back to OpenClaw via <code>exec.approval.resolve</code></li> <li>If no response within 55 seconds \u2192 auto-deny</li> </ol> <p>Defense in depth</p> <p>Even if someone bypasses the voice pre-filter (e.g., by crafting a prompt that doesn't trigger regex patterns), the post-filter catches the actual dangerous action at execution time.</p>"},{"location":"features/voice-security/#telegram-setup","title":"Telegram Setup","text":""},{"location":"features/voice-security/#create-a-bot","title":"Create a Bot","text":"<ol> <li>Message @BotFather on Telegram</li> <li>Send <code>/newbot</code> and follow the prompts</li> <li>Copy the bot token</li> </ol>"},{"location":"features/voice-security/#get-your-chat-id","title":"Get Your Chat ID","text":"<ol> <li>Message your new bot</li> <li>Visit <code>https://api.telegram.org/bot&lt;TOKEN&gt;/getUpdates</code></li> <li>Find <code>\"chat\":{\"id\":123456789}</code> in the response</li> </ol>"},{"location":"features/voice-security/#configure","title":"Configure","text":"<pre><code># .env\nKIWI_TELEGRAM_BOT_TOKEN=123456:ABC-DEF1234ghIkl-zyx57W2v1u123ew11\nKIWI_TELEGRAM_CHAT_ID=123456789\n</code></pre> <pre><code># config.yaml\nsecurity:\n  telegram_approval_enabled: true\n</code></pre>"},{"location":"features/voice-security/#approval-flow","title":"Approval Flow","text":"<p>When a non-owner says something dangerous:</p> <ol> <li>Kiwi announces: \"[Speaker name] wants to: [command]. Waiting for approval.\"</li> <li>Owner receives a Telegram message with the command text and two buttons: Approve / Deny</li> <li>Owner taps a button \u2192 Kiwi executes or rejects the command</li> <li>If no response \u2192 auto-deny after timeout</li> </ol>"},{"location":"features/wake-word/","title":"Wake Word Detection","text":"<p>Kiwi supports two wake word engines: text-based fuzzy matching (default) and OpenWakeWord ML detection.</p>"},{"location":"features/wake-word/#text-engine-default","title":"Text Engine (default)","text":"<p>The simplest approach \u2014 Kiwi runs Whisper STT continuously and checks the transcript for the wake word using fuzzy string matching.</p> <pre><code>wake_word:\n  engine: \"text\"\n  keyword: \"kiwi\"\n</code></pre> <p>How it works:</p> <ol> <li>Audio is captured and processed through Silero VAD (Voice Activity Detection)</li> <li>Speech segments are transcribed by Faster Whisper</li> <li>The transcript is checked for the wake word using Levenshtein distance</li> <li>Common misspellings and phonetic variants are defined per-language in locale files</li> </ol> <p>Pros: No extra model, works with any word, language-aware typo matching.</p> <p>Cons: Higher CPU/GPU usage (Whisper runs on all speech), slightly higher latency.</p>"},{"location":"features/wake-word/#openwakeword-engine-ml","title":"OpenWakeWord Engine (ML)","text":"<p>A dedicated ONNX model (~10MB) that listens to raw audio in real time \u2014 no Whisper needed for wake word detection.</p> <pre><code>wake_word:\n  engine: \"openwakeword\"\n  model: \"hey_jarvis\"      # Built-in model name or path to .onnx file\n  threshold: 0.5           # Detection sensitivity (0.0\u20131.0)\n</code></pre> <p>Performance: ~80ms latency, ~2% CPU usage.</p>"},{"location":"features/wake-word/#built-in-models","title":"Built-in Models","text":"Model Wake Phrase <code>hey_jarvis</code> \"Hey Jarvis\" <code>alexa</code> \"Alexa\" <code>hey_mycroft</code> \"Hey Mycroft\""},{"location":"features/wake-word/#how-it-works","title":"How it Works","text":"<ol> <li>Audio is captured at 16kHz</li> <li>OpenWakeWord processes raw PCM frames through a small ONNX neural network</li> <li>When confidence exceeds the threshold, wake word is detected</li> <li>Only then does Whisper STT activate to transcribe the actual command</li> <li>This saves significant compute \u2014 Whisper only runs when needed</li> </ol> <p>Threshold tuning</p> <p>Start with <code>0.5</code>. Lower values (0.3) increase sensitivity but may cause false activations. Higher values (0.7) reduce false positives but may miss some activations.</p>"},{"location":"features/wake-word/#train-a-custom-wake-word","title":"Train a Custom Wake Word","text":"<p>You can train your own wake word (e.g., \"Hey Kiwi\") using Google Colab \u2014 no voice recordings needed:</p> <pre><code>python scripts/train_wake_word.py --phrase \"hey kiwi\"\n</code></pre> <p>This generates synthetic training data and trains an ONNX model. Place the resulting <code>.onnx</code> file in the project directory and reference it in config:</p> <pre><code>wake_word:\n  engine: \"openwakeword\"\n  model: \"path/to/hey_kiwi.onnx\"\n  threshold: 0.5\n</code></pre>"},{"location":"features/wake-word/#environment-overrides","title":"Environment Overrides","text":"<pre><code>KIWI_WAKE_ENGINE=openwakeword\nKIWI_WAKE_MODEL=hey_jarvis\nKIWI_WAKE_THRESHOLD=0.5\n</code></pre>"},{"location":"features/wake-word/#comparison","title":"Comparison","text":"Text Engine OpenWakeWord CPU usage Higher (Whisper always on) ~2% Latency ~500ms\u20132s ~80ms Custom words Any word, just change config Needs model training Extra model No ~10MB ONNX Language support All 15 languages Language-independent"},{"location":"features/web-dashboard/","title":"Web Dashboard","text":"<p>Kiwi includes a real-time web dashboard served at <code>http://localhost:7789</code>.</p> <p></p>"},{"location":"features/web-dashboard/#features","title":"Features","text":"<ul> <li>Live state orb \u2014 animated indicator that changes color and pulse speed with assistant state (idle / listening / thinking / speaking)</li> <li>Metric tiles \u2014 real-time display of state, speaking status, processing status, active speaker, TTS provider, active soul, and exec approval status</li> <li>Event log \u2014 terminal-style feed of all system events via WebSocket</li> <li>Personality carousel \u2014 horizontal card carousel with holographic accents; click to activate a soul, NSFW souls highlighted in ruby</li> <li>Speaker management \u2014 table with voiceprint priority badges, block/unblock/delete actions</li> <li>Controls \u2014 stop playback, reset context, restart/shutdown, TTS test input</li> <li>Language switcher \u2014 change locale on the fly via dropdown</li> <li>Web Microphone \u2014 talk to Kiwi directly from the browser (see Web Microphone)</li> <li>Configuration view \u2014 current wake word, STT engine, TTS provider, LLM model, sample rate</li> </ul>"},{"location":"features/web-dashboard/#design","title":"Design","text":"<p>The dashboard uses a glassmorphism dark theme with:</p> <ul> <li>Frosted-glass panels with backdrop blur</li> <li>Panel accent gradient stripes color-coded by section</li> <li>Staggered entrance animations</li> <li>Dot grid background texture with ambient color glows</li> <li>Plus Jakarta Sans + JetBrains Mono fonts</li> </ul>"},{"location":"features/web-dashboard/#configuration","title":"Configuration","text":"<pre><code>api:\n  enabled: true\n  host: \"0.0.0.0\"\n  port: 7789\n</code></pre> <p>The dashboard is served by the same aiohttp server as the REST API. Disable the API to disable the dashboard.</p>"},{"location":"features/web-dashboard/#tech-stack","title":"Tech Stack","text":"<ul> <li>Vanilla HTML / CSS / JS (no framework, no build step)</li> <li>aiohttp for HTTP and WebSocket</li> <li>WebSocket connection for real-time EventBus streaming</li> <li>Responsive design with breakpoints at 960px, 768px, and 480px</li> </ul>"},{"location":"features/web-microphone/","title":"Web Microphone","text":"<p>The dashboard includes a Web Microphone that lets you talk to Kiwi directly from the browser \u2014 no local microphone setup or pyaudio installation needed.</p>"},{"location":"features/web-microphone/#how-it-works","title":"How it Works","text":"<ol> <li>Browser captures audio via AudioWorklet \u2014 low-latency audio processing off the main thread</li> <li>PCM Int16 audio is streamed to Kiwi over a WebSocket (<code>/api/audio</code>)</li> <li>Kiwi processes it through the same pipeline: STT \u2192 wake word \u2192 Speaker ID \u2192 LLM \u2192 TTS</li> <li>TTS responses are streamed back to the browser and played via AudioWorklet</li> <li>Live volume bars show audio level in real time</li> </ol>"},{"location":"features/web-microphone/#usage","title":"Usage","text":"<p>Click the microphone button in the dashboard to connect. The button animates with concentric rings while active. Volume bars show live input level.</p> <p>Tip</p> <p>The browser will ask for microphone permission on first use. Make sure to allow it.</p>"},{"location":"features/web-microphone/#configuration","title":"Configuration","text":"<pre><code>web_audio:\n  enabled: true\n  sample_rate: 16000\n  max_clients: 3\n</code></pre> Field Description <code>enabled</code> Enable/disable web audio streaming <code>sample_rate</code> Audio sample rate in Hz (default: 16000) <code>max_clients</code> Maximum simultaneous browser connections"},{"location":"features/web-microphone/#websocket-protocol","title":"WebSocket Protocol","text":"<p>The web audio endpoint is at <code>ws://localhost:7789/api/audio</code>.</p> <p>Client \u2192 Server: Raw PCM Int16 audio frames as binary WebSocket messages.</p> <p>Server \u2192 Client: TTS audio frames as binary WebSocket messages for browser playback.</p>"},{"location":"features/web-microphone/#requirements","title":"Requirements","text":"<ul> <li>Modern browser with AudioWorklet support (Chrome, Firefox, Edge, Safari 14.1+)</li> <li>HTTPS or localhost (AudioWorklet requires a secure context)</li> <li>Microphone permission granted</li> </ul>"},{"location":"getting-started/configuration/","title":"Configuration","text":"<p>Kiwi Voice is configured through two files:</p> File Purpose <code>config.yaml</code> All settings \u2014 language, STT, TTS, wake word, security, API <code>.env</code> Secrets and provider overrides (API keys, tokens) <p>Precedence: <code>config.yaml</code> \u2192 environment variables (<code>.env</code>) \u2192 hardcoded defaults.</p>"},{"location":"getting-started/configuration/#configyaml","title":"config.yaml","text":""},{"location":"getting-started/configuration/#language","title":"Language","text":"<pre><code>language: \"en\"   # en, ru, es, pt, fr, it, de, tr, pl, zh, ja, ko, hi, ar, id\n</code></pre> <p>Controls all user-facing strings, STT language, TTS voice selection, wake word variants, and security patterns.</p>"},{"location":"getting-started/configuration/#wake-word","title":"Wake Word","text":"<pre><code>wake_word:\n  engine: \"text\"             # text (fuzzy match) or openwakeword (ML model)\n  keyword: \"kiwi\"\n  model: \"hey_jarvis\"        # OpenWakeWord model name or path to .onnx\n  threshold: 0.5             # Detection sensitivity (0.0\u20131.0)\n</code></pre> <p>See Wake Word Detection for details on both engines.</p>"},{"location":"getting-started/configuration/#speech-to-text","title":"Speech-to-Text","text":"<pre><code>stt:\n  engine: \"faster-whisper\"   # faster-whisper or mlx-whisper (Apple Silicon)\n  model: \"small\"             # tiny, base, small, medium, large\n  device: \"cuda\"             # cuda or cpu\n  compute_type: \"float16\"    # float16 (GPU) or int8 (CPU)\n</code></pre> <p>Model size tradeoff</p> <p><code>small</code> is the sweet spot \u2014 fast with good accuracy. Use <code>large</code> for best accuracy (slower startup), <code>tiny</code> for minimal resources.</p>"},{"location":"getting-started/configuration/#text-to-speech","title":"Text-to-Speech","text":"<pre><code>tts:\n  provider: \"kokoro\"         # kokoro, piper, qwen3, elevenlabs\n  elevenlabs:\n    voice_id: \"aEO01A4wXwd1O8GPgGlF\"\n    model_id: \"eleven_multilingual_v2\"\n    stability: 0.45\n    similarity_boost: 0.75\n    speed: 1.0\n  kokoro:\n    voice: \"af_heart\"        # 14 voices available\n    speed: 1.0\n  piper:\n    model: \"en_US-lessac-medium\"\n  qwen3:\n    backend: \"local\"         # local or runpod\n</code></pre> <p>See TTS Providers for a full comparison.</p>"},{"location":"getting-started/configuration/#speaker-priority","title":"Speaker Priority","text":"<pre><code>speaker_priority:\n  owner:\n    name: \"Owner\"            # Change to your name\n</code></pre>"},{"location":"getting-started/configuration/#voice-security","title":"Voice Security","text":"<pre><code>security:\n  telegram_approval_enabled: true\n</code></pre>"},{"location":"getting-started/configuration/#llm","title":"LLM","text":"<pre><code>llm:\n  model: \"openai/gpt-4o\"\n  chat_timeout: 120\n</code></pre>"},{"location":"getting-started/configuration/#audio-devices","title":"Audio Devices","text":"<pre><code>audio:\n  output_device: null        # null = system default\n  input_device: null         # null = system default\n</code></pre> <p>List available devices:</p> <pre><code>python -c \"import sounddevice; print(sounddevice.query_devices())\"\n</code></pre>"},{"location":"getting-started/configuration/#rest-api","title":"REST API","text":"<pre><code>api:\n  enabled: true\n  host: \"0.0.0.0\"\n  port: 7789\n</code></pre>"},{"location":"getting-started/configuration/#web-audio","title":"Web Audio","text":"<pre><code>web_audio:\n  enabled: true\n  sample_rate: 16000\n  max_clients: 3\n</code></pre>"},{"location":"getting-started/configuration/#home-assistant","title":"Home Assistant","text":"<pre><code>homeassistant:\n  enabled: true\n  url: \"http://homeassistant.local:8123\"\n  token: \"\"                  # Long-Lived Access Token\n</code></pre>"},{"location":"getting-started/configuration/#souls-personalities","title":"Souls (Personalities)","text":"<pre><code>souls:\n  default: \"mindful-companion\"\n  nsfw:\n    model: \"openrouter/mistralai/mistral-7b-instruct\"\n    session: \"kiwi-nsfw\"\n</code></pre>"},{"location":"getting-started/configuration/#environment-variables","title":"Environment Variables","text":"<p>All settings can be overridden via environment variables in <code>.env</code>:</p> Variable Description <code>KIWI_LANGUAGE</code> Override language (<code>en</code>, <code>ru</code>, etc.) <code>KIWI_TTS_PROVIDER</code> Override TTS provider (<code>kokoro</code>, <code>piper</code>, <code>qwen3</code>, <code>elevenlabs</code>) <code>KIWI_ELEVENLABS_API_KEY</code> ElevenLabs API key <code>RUNPOD_API_KEY</code> RunPod API key (Qwen3-TTS serverless) <code>RUNPOD_TTS_ENDPOINT_ID</code> RunPod endpoint ID <code>KIWI_TELEGRAM_BOT_TOKEN</code> Telegram bot token (voice security) <code>KIWI_TELEGRAM_CHAT_ID</code> Telegram chat ID for approvals <code>KIWI_WAKE_ENGINE</code> Override wake word engine (<code>text</code>, <code>openwakeword</code>) <code>KIWI_WAKE_MODEL</code> Override OpenWakeWord model <code>KIWI_WAKE_THRESHOLD</code> Override detection threshold <code>KIWI_STT_ENGINE</code> Override STT engine (<code>faster-whisper</code>, <code>mlx-whisper</code>) <code>KIWI_FFMPEG_PATH</code> Custom FFmpeg path <code>KIWI_DEBUG</code> Enable debug logging <code>LLM_MODEL</code> Override LLM model <p>See <code>.env.example</code> in the repository for the full list.</p>"},{"location":"getting-started/first-run/","title":"First Run","text":""},{"location":"getting-started/first-run/#start-kiwi","title":"Start Kiwi","text":"Linux / macOSWindows (PowerShell)Windows (Git Bash) <pre><code>source venv/bin/activate\npython -m kiwi\n</code></pre> <pre><code>.\\start.ps1\n# or\n.\\venv\\Scripts\\activate\npython -m kiwi\n</code></pre> <pre><code>source venv/Scripts/activate\npython -m kiwi\n</code></pre> <p>You should see startup logs like:</p> <pre><code>[14:08:25.342] [INFO] [CORE] Kiwi Voice starting...\n[14:08:25.500] [INFO] [STT] Loading Faster Whisper (small, cuda)...\n[14:08:26.100] [INFO] [TTS] Kokoro ONNX initialized (14 voices)\n[14:08:26.200] [INFO] [API] Dashboard at http://localhost:7789\n[14:08:26.300] [INFO] [CORE] Listening...\n</code></pre>"},{"location":"getting-started/first-run/#open-the-dashboard","title":"Open the Dashboard","text":"<p>Navigate to http://localhost:7789 in your browser. You'll see the real-time dashboard with live status, event log, controls, and more.</p> <p></p>"},{"location":"getting-started/first-run/#register-your-voice","title":"Register Your Voice","text":"<p>Say the wake word followed by the registration command:</p> <p>\"Kiwi, remember my voice\"</p> <p>Kiwi will capture your voiceprint and register you as the Owner \u2014 the highest priority level with full access.</p> <p>Important</p> <p>Register your voice first. Until you do, all speakers are treated as guests and may need Telegram approval for certain commands.</p>"},{"location":"getting-started/first-run/#try-a-command","title":"Try a Command","text":"<p>Once registered, try:</p> <p>\"Kiwi, what time is it?\"</p> <p>The flow:</p> <ol> <li>Wake word \"kiwi\" is detected (text fuzzy match or ML model)</li> <li>Your speech is transcribed by Faster Whisper</li> <li>Speaker ID confirms you're the Owner</li> <li>Command is sent to OpenClaw via WebSocket</li> <li>LLM response streams back</li> <li>TTS speaks the answer</li> </ol>"},{"location":"getting-started/first-run/#use-the-web-microphone","title":"Use the Web Microphone","text":"<p>If you don't have a local microphone (or don't want to install pyaudio), click the microphone button in the dashboard to talk to Kiwi directly from the browser.</p>"},{"location":"getting-started/first-run/#whats-next","title":"What's Next","text":"<ul> <li>Wake Word Detection \u2014 switch to ML-based detection</li> <li>TTS Providers \u2014 choose your preferred voice engine</li> <li>Voice Security \u2014 set up Telegram approvals</li> <li>Home Assistant \u2014 control your smart home by voice</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.10+</li> <li>FFmpeg \u2014 for audio processing</li> <li>OpenClaw running locally</li> <li>GPU with CUDA recommended for STT and local TTS, but not required</li> </ul>"},{"location":"getting-started/installation/#clone-install","title":"Clone &amp; Install","text":"Linux / macOSWindows <pre><code>git clone https://github.com/ekleziast/kiwi-voice.git\ncd kiwi-voice\n\npython -m venv venv\nsource venv/bin/activate\n\npip install -r requirements.txt\n</code></pre> <pre><code>git clone https://github.com/ekleziast/kiwi-voice.git\ncd kiwi-voice\n\npython -m venv venv\nvenv\\Scripts\\activate\n\npip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#ffmpeg","title":"FFmpeg","text":"LinuxmacOSWindows <pre><code>sudo apt install ffmpeg\n</code></pre> <pre><code>brew install ffmpeg\n</code></pre> <p>Download from ffmpeg.org and add to PATH, or set <code>KIWI_FFMPEG_PATH</code> in <code>.env</code>.</p>"},{"location":"getting-started/installation/#environment-file","title":"Environment File","text":"<pre><code>cp .env.example .env\n</code></pre> <p>Edit <code>.env</code> with your API keys. All keys are optional \u2014 Kiwi works with free local providers out of the box:</p> <pre><code># Optional: ElevenLabs for cloud TTS\nKIWI_ELEVENLABS_API_KEY=sk-...\n\n# Optional: Telegram bot for voice security approvals\nKIWI_TELEGRAM_BOT_TOKEN=123456:ABC...\nKIWI_TELEGRAM_CHAT_ID=123456789\n\n# Optional: RunPod for Qwen3-TTS serverless\nRUNPOD_API_KEY=...\nRUNPOD_TTS_ENDPOINT_ID=...\n</code></pre> <p>Zero-cost setup</p> <p>Kiwi works fully local and free with Kokoro ONNX (TTS) + Faster Whisper (STT) + text wake word engine. No API keys needed.</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Configuration \u2014 customize <code>config.yaml</code></li> <li>First Run \u2014 start Kiwi and register your voice</li> </ul>"}]}