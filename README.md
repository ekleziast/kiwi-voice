<p align="center">
  <img src="https://em-content.zobj.net/source/apple/391/kiwi-fruit_1f95d.png" width="120" alt="Kiwi Voice">
</p>

<h1 align="center">Kiwi Voice</h1>

<p align="center">
  <strong>OpenClaw voice assistant â€” speaker ID, voice-gated command approval, barge-in interrupts, and sentence-aware streaming TTS</strong>
</p>

<p align="center">
  <a href="https://github.com/ekleziast/kiwi-voice/blob/main/LICENSE"><img src="https://img.shields.io/badge/license-MIT-blue.svg" alt="License: MIT"></a>
  <a href="https://www.python.org/downloads/"><img src="https://img.shields.io/badge/python-3.10%2B-blue.svg" alt="Python 3.10+"></a>
  <a href="https://github.com/openclaw/openclaw"><img src="https://img.shields.io/badge/backend-OpenClaw-orange.svg" alt="OpenClaw"></a>
</p>

<p align="center">
  <a href="README.ru.md">ğŸ‡·ğŸ‡º Ğ”Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ğ¸Ñ Ğ½Ğ° Ñ€ÑƒÑÑĞºĞ¾Ğ¼</a>
</p>

---

## What is Kiwi Voice?

Kiwi Voice is a real-time voice interface that turns [OpenClaw](https://github.com/openclaw/openclaw) into a hands-free assistant. It captures audio from your microphone, recognizes speech locally via Faster Whisper, identifies *who* is speaking, enforces voice-based security policies, talks to any LLM through OpenClaw's WebSocket gateway, and speaks the response back â€” all in a continuous loop.

Think of it as Alexa/Siri, but self-hosted, privacy-first, and plugged into your own AI stack.

### Key Features

| Feature | Description |
|---------|-------------|
| ğŸ—£ï¸ **Wake Word** | Activate with a configurable keyword (default: *"kiwi"*) |
| ğŸ­ **Speaker ID** | Voiceprint recognition via pyannote embeddings â€” knows who's talking |
| ğŸ” **Voice Security** | Priority hierarchy (Owner â†’ Friend â†’ Guest â†’ Blocked) with Telegram approval for dangerous commands |
| ğŸ”Š **Multi-Provider TTS** | ElevenLabs (cloud), Piper (local/free), Qwen3-TTS (local GPU / RunPod serverless) |
| âš¡ **Streaming TTS** | Sentence-aware chunking â€” starts speaking before the LLM finishes |
| ğŸ›‘ **Barge-In** | Interrupt the assistant mid-sentence by speaking over it |
| ğŸ§  **Auto-Learning** | Automatically remembers new voices after first interaction |
| ğŸ”Œ **WebSocket** | Native OpenClaw Gateway v3 protocol with delta/final streaming |
| ğŸŒ **Multi-Language** | Built-in i18n with YAML locale files â€” switch language with a single config field |

## Architecture

```
Mic â†’ VAD + Energy Detection â†’ Faster Whisper STT â†’ Wake Word Check
  â†’ Speaker ID (pyannote) â†’ Priority Gate â†’ Voice Security
  â†’ OpenClaw Gateway (WebSocket) â†’ LLM response stream
  â†’ Real-time streaming TTS â†’ Speaker Output (with barge-in)
  â†’ Back to listening
```

## Quick Start

### Requirements

- **Python 3.10+**
- **FFmpeg** (for audio processing)
- **[OpenClaw](https://github.com/openclaw/openclaw)** running locally
- **GPU with CUDA** recommended (for STT & local TTS), but not required

### Installation

```bash
git clone https://github.com/ekleziast/kiwi-voice.git
cd kiwi-voice

python -m venv venv
# Linux / macOS
source venv/bin/activate
# Windows
venv\Scripts\activate

pip install -r requirements.txt
```

### Configuration

```bash
cp .env.example .env
# Fill in your API keys (ElevenLabs, RunPod, Telegram â€” all optional)
```

Edit `config.yaml` to match your setup:

```yaml
# Language: controls UI strings, STT, TTS, wake word, and command patterns
language: "ru"               # ru | en (add more in kiwi/locales/)

# TTS provider: elevenlabs | piper | qwen3
tts:
  provider: "piper"          # Free, local, no API key needed

# STT model
stt:
  model: "small"             # small = fast, large = accurate
  device: "cuda"             # cuda | cpu

# Wake word
wake_word:
  keyword: "kiwi"

# Owner name (used for voice commands like "I'm <name>")
speaker_priority:
  owner:
    name: "Owner"            # Change to your name
```

### Run

```bash
python -m kiwi
```

Or use the launcher scripts:

```bash
# Windows
start.bat
.\start.ps1

# Linux / macOS
python -m kiwi
```

## TTS Providers

| Provider | Quality | Latency | Cost | Local GPU |
|----------|---------|---------|------|-----------|
| **ElevenLabs** | Excellent | ~0.3â€“0.5s | ~$0.30/1K chars | No |
| **Qwen3-TTS (local)** | High | ~1â€“3s | Free | Yes (CUDA) |
| **Qwen3-TTS (RunPod)** | High | ~2â€“5s | ~$0.0003/sec | No |
| **Piper** | Good | <0.5s | Free | No |

Switch providers in `config.yaml` or via environment variable:

```bash
KIWI_TTS_PROVIDER=piper python -m kiwi
```

## Voice Security

Kiwi identifies speakers by voiceprint and enforces a priority hierarchy:

```
OWNER (priority 0)   â€” Full access, cannot be blocked
FRIEND (priority 1)  â€” Dangerous commands require Telegram approval
GUEST (priority 2)   â€” All sensitive commands require approval
BLOCKED (priority 99) â€” Completely ignored
```

### Voice Commands

| Command | Action |
|---------|--------|
| *"Kiwi, remember my voice"* | Register your voiceprint as owner |
| *"Kiwi, this is my friend [name]"* | Add someone as a friend |
| *"Kiwi, block them"* | Block the last speaker |
| *"Kiwi, who is speaking?"* | Identify the current speaker |
| *"Kiwi, what voices do you know?"* | List all known voiceprints |

> ğŸ’¡ Voice commands are language-dependent. Set `language` in `config.yaml` to match your locale. See `kiwi/locales/*.yaml` for the full command lists.

### Telegram Approval

When a non-owner speaker issues a potentially dangerous command, Kiwi sends a confirmation request to the owner via Telegram. The owner can approve or deny it from their phone.

Set `KIWI_TELEGRAM_BOT_TOKEN` and `KIWI_TELEGRAM_CHAT_ID` in `.env` to enable.

## Environment Variables

| Variable | Description |
|----------|-------------|
| `KIWI_ELEVENLABS_API_KEY` | ElevenLabs API key |
| `RUNPOD_API_KEY` | RunPod API key (for Qwen3-TTS serverless) |
| `RUNPOD_TTS_ENDPOINT_ID` | RunPod endpoint ID |
| `KIWI_TELEGRAM_BOT_TOKEN` | Telegram bot token (voice security) |
| `KIWI_TELEGRAM_CHAT_ID` | Telegram chat ID for approval messages |
| `KIWI_TTS_PROVIDER` | Override TTS provider |
| `KIWI_FFMPEG_PATH` | Custom FFmpeg path |
| `KIWI_LANGUAGE` | Override language/locale (`ru`, `en`, etc.) |
| `KIWI_DEBUG` | Enable debug logging |
| `LLM_MODEL` | Override LLM model |

See `.env.example` for the full list.

## Project Structure

```
kiwi-voice/
â”œâ”€â”€ kiwi/                    # Main Python package
â”‚   â”œâ”€â”€ service.py           # Core orchestrator
â”‚   â”œâ”€â”€ listener.py          # Audio capture, Whisper STT, wake word, VAD
â”‚   â”œâ”€â”€ speaker_id.py        # Voiceprint extraction (pyannote)
â”‚   â”œâ”€â”€ speaker_manager.py   # Priority hierarchy + hot cache
â”‚   â”œâ”€â”€ voice_security.py    # Dangerous command detection + Telegram approval
â”‚   â”œâ”€â”€ openclaw_ws.py       # WebSocket client (OpenClaw Gateway v3)
â”‚   â”œâ”€â”€ config_loader.py     # YAML/env config loading
â”‚   â”œâ”€â”€ text_processing.py   # Text cleanup and sentence splitting for TTS
â”‚   â”œâ”€â”€ unified_vad.py       # Voice Activity Detection
â”‚   â”œâ”€â”€ hardware_aec.py      # Acoustic Echo Cancellation
â”‚   â”œâ”€â”€ task_announcer.py    # Long-running task status announcer
â”‚   â”œâ”€â”€ i18n.py              # Lightweight i18n module (YAML locale loader)
â”‚   â”œâ”€â”€ locales/             # Locale files (ru.yaml, en.yaml, ...)
â”‚   â”œâ”€â”€ mixins/              # Service mixin modules
â”‚   â”‚   â”œâ”€â”€ audio_playback.py    # Audio output and playback control
â”‚   â”‚   â”œâ”€â”€ dialogue_pipeline.py # LLM dialogue orchestration
â”‚   â”‚   â”œâ”€â”€ llm_callbacks.py     # LLM streaming callbacks
â”‚   â”‚   â”œâ”€â”€ stream_watchdog.py   # Stream stall detection and recovery
â”‚   â”‚   â””â”€â”€ tts_speech.py        # TTS synthesis and streaming playback
â”‚   â””â”€â”€ tts/                 # TTS providers
â”‚       â”œâ”€â”€ elevenlabs.py
â”‚       â”œâ”€â”€ elevenlabs_ws.py  # WebSocket input streaming
â”‚       â”œâ”€â”€ piper.py
â”‚       â”œâ”€â”€ qwen_local.py
â”‚       â”œâ”€â”€ runpod.py
â”‚       â””â”€â”€ streaming.py     # Sentence-aware streaming manager
â”œâ”€â”€ runpod/                  # RunPod serverless deployment (Qwen3-TTS)
â”œâ”€â”€ scripts/                 # Utility scripts
â”œâ”€â”€ sounds/                  # Audio assets (startup, confirmation, idle)
â”œâ”€â”€ tests/                   # Smoke tests
â”œâ”€â”€ config.yaml              # Main configuration
â”œâ”€â”€ .env.example             # Secret template
â””â”€â”€ pyproject.toml           # Package metadata
```

## Development

```bash
# Run tests
pytest tests/

# Code conventions:
# - Logging: kiwi_log("TAG", "message", level="INFO") â€” never print()
# - Paths: PROJECT_ROOT from kiwi package
# - Optional modules: try/except + *_AVAILABLE flags
# - Threads: daemon threads + crash protection
# - GPU: auto-detect CUDA with CPU fallback
```

## Multi-Language Support

Kiwi uses YAML-based locale files in `kiwi/locales/`. All user-facing strings, voice commands, wake word variants, hallucination filters, and security patterns are loaded from locale files.

**Switch language:**
```yaml
# config.yaml
language: "en"   # or "ru", etc.
```

**Add a new language:**
1. Copy `kiwi/locales/en.yaml` to `kiwi/locales/{lang}.yaml`
2. Translate all strings
3. Set `language: "{lang}"` in `config.yaml`

Currently shipped â€” **15 languages:**

| | | | |
|---|---|---|---|
| `ru` Russian | `en` English | `es` Spanish | `pt` Portuguese |
| `fr` French | `it` Italian | `de` German | `tr` Turkish |
| `pl` Polish | `zh` Chinese | `ja` Japanese | `ko` Korean |
| `hi` Hindi | `ar` Arabic | `id` Indonesian | |

## Roadmap

- [ ] Web UI for configuration
- [ ] Plugin system for custom wake words
- [ ] Home Assistant integration

## License

[MIT](LICENSE) â€” do whatever you want with it.

---

<p align="center">
  Built with ğŸ¥ and too much coffee
</p>
